import asyncio
import json
import logging
from typing import Any, Awaitable, Callable, Dict, Generator, List, Optional, Tuple, TypeVar

from prometheus_client import Histogram, exposition  # type: ignore
from prometheus_client.core import (  # type: ignore
    REGISTRY,
    CounterMetricFamily,
    GaugeMetricFamily,
    HistogramMetricFamily,
    Metric,
)

from knot_resolver_manager.datamodel.config_schema import KresConfig
from knot_resolver_manager.kres_id import KresID
from knot_resolver_manager.kresd_controller.interface import Subprocess

logger = logging.getLogger(__name__)

MANAGER_REQUEST_RECONFIGURE_LATENCY = Histogram(
    "manager_request_reconfigure_latency", "Time it takes to change configuration"
)

_REGISTERED_RESOLVERS: Dict[KresID, Subprocess] = {}


T = TypeVar("T")


def async_timing_histogram(metric: Histogram) -> Callable[[Callable[..., Awaitable[T]]], Callable[..., Awaitable[T]]]:
    """
    Decorator which can be used to report duration on async functions
    """

    def decorator(func: Callable[..., Awaitable[T]]) -> Callable[..., Awaitable[T]]:
        async def wrapper(*args: Any, **kwargs: Any) -> T:
            with metric.time():
                res = await func(*args, **kwargs)
                return res

        return wrapper

    return decorator


async def _command_registered_resolvers(cmd: str) -> Dict[KresID, str]:
    async def single_pair(sub: Subprocess) -> Tuple[KresID, str]:
        return sub.id, await sub.command(cmd)

    pairs = await asyncio.gather(*(single_pair(inst) for inst in _REGISTERED_RESOLVERS.values()))
    return dict(pairs)


def _counter(name: str, description: str, label: Tuple[str, str], value: float) -> CounterMetricFamily:
    c = CounterMetricFamily(name, description, labels=(label[0],))
    c.add_metric(label[1], value)  # type: ignore
    return c


def _gauge(name: str, description: str, label: Tuple[str, str], value: float) -> GaugeMetricFamily:
    c = GaugeMetricFamily(name, description, labels=(label[0],))
    c.add_metric(label[1], value)  # type: ignore
    return c


def _histogram(
    name: str, description: str, label: Tuple[str, str], buckets: List[Tuple[str, int]], sum_value: float
) -> HistogramMetricFamily:
    c = HistogramMetricFamily(name, description, labels=(label[0],))
    c.add_metric(label[1], buckets, sum_value=sum_value)  # type: ignore
    return c


class ResolverCollector:
    def __init__(self) -> None:
        self._stats_raw: Optional[Dict[KresID, str]] = None

    def set_stats(self, stats_raw: Optional[Dict[KresID, str]]) -> None:
        self._stats_raw = stats_raw

    def collect(self) -> Generator[Metric, None, None]:
        if self._stats_raw is None:
            return

        for kid, raw in self._stats_raw.items():
            success = False
            try:
                metrics: Dict[str, int] = json.loads(raw[1:-1])
                yield from self._parse_resolver_metrics(kid, metrics)
                success = True
            except json.JSONDecodeError:
                logger.warning("Failed to load metrics from resolver instance %s: failed to parse statistics", str(kid))
            except KeyError as e:
                logger.warning(
                    "Failed to load metrics from resolver instance %s: attempted to read missing statistic %s",
                    str(kid),
                    str(e),
                )

            yield _gauge(
                "resolver_metrics_loaded",
                "0 if metrics from resolver instance were not loaded, otherwise 1",
                label=("instance_id", str(kid)),
                value=int(success),
            )

    def _parse_resolver_metrics(self, instance_id: KresID, metrics: Any) -> Generator[Metric, None, None]:
        sid = str(instance_id)

        # response latency histogram
        BUCKET_NAMES_IN_RESOLVER = ("1ms", "10ms", "50ms", "100ms", "250ms", "500ms", "1000ms", "1500ms", "slow")
        BUCKET_NAMES_PROMETHEUS = ("0.001", "0.01", "0.05", "0.1", "0.25", "0.5", "1.0", "1.5", "+Inf")
        yield _histogram(
            "resolver_response_latency",
            "Time it takes to respond to queries in seconds",
            label=("instance_id", sid),
            buckets=[
                (bnp, metrics[f"answer.{duration}"])
                for bnp, duration in zip(BUCKET_NAMES_PROMETHEUS, BUCKET_NAMES_IN_RESOLVER)
            ],
            sum_value=metrics["answer.sum_ms"] / 1_000,
        )

        yield _counter(
            "resolver_request_total",
            "total number of DNS requests (including internal client requests)",
            label=("instance_id", sid),
            value=metrics["request.total"],
        )
        yield _counter(
            "resolver_request_internal",
            "number of internal requests generated by Knot Resolver (e.g. DNSSEC trust anchor updates)",
            label=("instance_id", sid),
            value=metrics["request.internal"],
        )
        yield _counter(
            "resolver_request_udp",
            "number of external requests received over plain UDP (RFC 1035)",
            label=("instance_id", sid),
            value=metrics["request.udp"],
        )
        yield _counter(
            "resolver_request_tcp",
            "number of external requests received over plain TCP (RFC 1035)",
            label=("instance_id", sid),
            value=metrics["request.tcp"],
        )
        yield _counter(
            "resolver_request_dot",
            "number of external requests received over DNS-over-TLS (RFC 7858)",
            label=("instance_id", sid),
            value=metrics["request.dot"],
        )
        yield _counter(
            "resolver_request_doh",
            "number of external requests received over DNS-over-HTTP (RFC 8484)",
            label=("instance_id", sid),
            value=metrics["request.doh"],
        )
        yield _counter(
            "resolver_request_xdp",
            "number of external requests received over plain UDP via an AF_XDP socket",
            label=("instance_id", sid),
            value=metrics["request.xdp"],
        )
        yield _counter(
            "resolver_answer_total",
            "total number of answered queries",
            label=("instance_id", sid),
            value=metrics["answer.total"],
        )
        yield _counter(
            "resolver_answer_cached",
            "number of queries answered from cache",
            label=("instance_id", sid),
            value=metrics["answer.cached"],
        )
        yield _counter(
            "resolver_answer_rcode_noerror",
            "number of NOERROR answers",
            label=("instance_id", sid),
            value=metrics["answer.noerror"],
        )
        yield _counter(
            "resolver_answer_rcode_nodata",
            "number of NOERROR answers without any data",
            label=("instance_id", sid),
            value=metrics["answer.nodata"],
        )
        yield _counter(
            "resolver_answer_rcode_nxdomain",
            "number of NXDOMAIN answers",
            label=("instance_id", sid),
            value=metrics["answer.nxdomain"],
        )
        yield _counter(
            "resolver_answer_rcode_servfail",
            "number of SERVFAIL answers",
            label=("instance_id", sid),
            value=metrics["answer.servfail"],
        )
        yield _counter(
            "resolver_answer_flag_aa",
            "number of authoritative answers",
            label=("instance_id", sid),
            value=metrics["answer.aa"],
        )
        yield _counter(
            "resolver_answer_flag_tc",
            "number of truncated answers",
            label=("instance_id", sid),
            value=metrics["answer.tc"],
        )
        yield _counter(
            "resolver_answer_flag_ra",
            "number of answers with recursion available flag",
            label=("instance_id", sid),
            value=metrics["answer.ra"],
        )
        yield _counter(
            "resolver_answer_flags_rd",
            "number of recursion desired (in answer!)",
            label=("instance_id", sid),
            value=metrics["answer.rd"],
        )
        yield _counter(
            "resolver_answer_flag_ad",
            "number of authentic data (DNSSEC) answers",
            label=("instance_id", sid),
            value=metrics["answer.ad"],
        )
        yield _counter(
            "resolver_answer_flag_cd",
            "number of checking disabled (DNSSEC) answers",
            label=("instance_id", sid),
            value=metrics["answer.cd"],
        )
        yield _counter(
            "resolver_answer_flag_do",
            "number of DNSSEC answer OK",
            label=("instance_id", sid),
            value=metrics["answer.do"],
        )
        yield _counter(
            "resolver_answer_flag_edns0",
            "number of answers with EDNS0 present",
            label=("instance_id", sid),
            value=metrics["answer.edns0"],
        )
        yield _counter(
            "resolver_query_edns",
            "number of queries with EDNS present",
            label=("instance_id", sid),
            value=metrics["query.edns"],
        )
        yield _counter(
            "resolver_query_dnssec",
            "number of queries with DNSSEC DO=1",
            label=("instance_id", sid),
            value=metrics["query.dnssec"],
        )


_RESOLVER_COLLECTOR = ResolverCollector()
REGISTRY.register(_RESOLVER_COLLECTOR)  # type: ignore


def unregister_resolver_metrics_for(subprocess: Subprocess) -> None:
    """
    Cancel metric collection from resolver subprocess
    """
    del _REGISTERED_RESOLVERS[subprocess.id]


def register_resolver_metrics_for(subprocess: Subprocess) -> None:
    """
    Register resolver subprocess for metric collection
    """
    _REGISTERED_RESOLVERS[subprocess.id] = subprocess


async def report_stats(config: KresConfig) -> bytes:
    """
    Collects metrics from everything, returns data string in Prometheus format.
    """

    try:
        if config.monitoring.state != "manager-only":
            lazy = config.monitoring.state == "lazy"
            ON_DEMAND_STATS_QUERY = "collect_lazy_statistics()"
            STATS_QUERY = "collect_statistics()"

            cmd = ON_DEMAND_STATS_QUERY if lazy else STATS_QUERY
            stats_raw = await _command_registered_resolvers(cmd)
            _RESOLVER_COLLECTOR.set_stats(stats_raw)

        return exposition.generate_latest()  # type: ignore
    finally:
        # after the report has been generated, clean everything
        _RESOLVER_COLLECTOR.set_stats(None)
